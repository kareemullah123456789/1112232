{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kareemullah123456789/1112232/blob/master/Chapter03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark --quiet"
      ],
      "metadata": {
        "id": "eDDzrDonlixi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91aff93c-a699-454d-a708-2a449a9bae04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPCtL7yjlok9",
        "outputId": "77e2c2b7-7a10-4cb6-cd09-92ca5109f7e6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.2.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2mn3JmxhS2Tva5L5LPlVPg8vENL_6F56KMq9DJaoH2bRUEhBz\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC1utT6Onq98",
        "outputId": "231bb89b-bd31-444c-fb27-fc9fa64e5525"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace \"your_auth_token\" with your actual ngrok token\n",
        "ngrok.set_auth_token(\"2mn3JmxhS2Tva5L5LPlVPg8vENL_6F56KMq9DJaoH2bRUEhBz\")"
      ],
      "metadata": {
        "id": "_gN5gtQ8ucH3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!killall ngrok # to kill multiple tunnels\n",
        "#sc.stop()"
      ],
      "metadata": {
        "id": "CoI5arwevGkF"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# # Initialize SparkContext\n",
        "# conf = SparkConf().setAppName(\"MyApp\").setMaster(\"local\")\n",
        "# sc = SparkContext(conf=conf)"
      ],
      "metadata": {
        "id": "jYTVSO2flxo-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set up Spark configuration\n",
        "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"films2\").setMaster(\"local[*]\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "# Expose the Spark UI port via ngrok\n",
        "spark_ui_url = ngrok.connect(4050)\n",
        "\n",
        "print(f\"Spark UI running on: {spark_ui_url}\")"
      ],
      "metadata": {
        "id": "K-K9EbeqlxuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc2091e-d7bc-4af9-9d5c-ce4647d64192"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark UI running on: NgrokTunnel: \"https://641e-34-44-118-71.ngrok-free.app\" -> \"http://localhost:4050\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Create an ngrok tunnel for port 4040\n",
        "public_url = ngrok.connect(4040)\n",
        "print(f\"Access Spark UI at: {public_url}\")\n"
      ],
      "metadata": {
        "id": "mLjn2UK4loyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0db639f-c15d-418c-b5e7-319da2687423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access Spark UI at: NgrokTunnel: \"https://2d1e-34-139-86-66.ngrok-free.app\" -> \"http://localhost:4040\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Va1jYX1SuZUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate your own DataFrame\n",
        "Instead of accessing the file system, let's create a DataFrame by generating the data.  In this case, we'll first create the `stringRDD` RDD and then convert it into a DataFrame when we're reading `stringJSONRDD` using `spark.read.json`."
      ],
      "metadata": {
        "id": "lQnaMvy6lgRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate our own JSON data\n",
        "#   This way we don't have to access the file system yet.\n",
        "stringJSONRDD = sc.parallelize((\"\"\"\n",
        "  { \"id\": \"123\",\n",
        "    \"name\": \"Katie\",\n",
        "    \"age\": 19,\n",
        "    \"eyeColor\": \"brown\"\n",
        "  }\"\"\",\n",
        "   \"\"\"{\n",
        "    \"id\": \"234\",\n",
        "    \"name\": \"Michael\",\n",
        "    \"age\": 22,\n",
        "    \"eyeColor\": \"green\"\n",
        "  }\"\"\",\n",
        "  \"\"\"{\n",
        "    \"id\": \"345\",\n",
        "    \"name\": \"Simone\",\n",
        "    \"age\": 23,\n",
        "    \"eyeColor\": \"blue\"\n",
        "  }\"\"\")\n",
        ")"
      ],
      "metadata": {
        "id": "ZjkmvQGblgRt"
      },
      "outputs": [],
      "execution_count": 43
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "xAu39qnDpBcZ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "swimmersJSON = spark.read.json(stringJSONRDD)"
      ],
      "metadata": {
        "id": "usBDfzNxlgRw"
      },
      "outputs": [],
      "execution_count": 45
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary table\n",
        "swimmersJSON.createOrReplaceTempView(\"swimmersJSON\")"
      ],
      "metadata": {
        "id": "5cp-MDYKlgRx"
      },
      "outputs": [],
      "execution_count": 46
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame API\n",
        "swimmersJSON.show()"
      ],
      "metadata": {
        "id": "dXTxMuOrlgRx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f4377ee-c8be-4dcd-ecc2-d0a53a0f59b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+---+-------+\n",
            "|age|eyeColor| id|   name|\n",
            "+---+--------+---+-------+\n",
            "| 19|   brown|123|  Katie|\n",
            "| 22|   green|234|Michael|\n",
            "| 23|    blue|345| Simone|\n",
            "+---+--------+---+-------+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 47
    },
    {
      "cell_type": "code",
      "source": [
        "# SQL Query\n",
        "spark.sql(\"select * from swimmersJSON\").collect()"
      ],
      "metadata": {
        "id": "C3LQp2T3lgRy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad9d0cf-4ee9-4364-f495-1ddcdedbcee3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(age=19, eyeColor='brown', id='123', name='Katie'),\n",
              " Row(age=22, eyeColor='green', id='234', name='Michael'),\n",
              " Row(age=23, eyeColor='blue', id='345', name='Simone')]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "execution_count": 48
    },
    {
      "cell_type": "code",
      "source": [
        "# %sql\n",
        "# -- Query Data\n",
        "# select * from swimmersJSON"
      ],
      "metadata": {
        "id": "_v6LQDWwlgRy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Inferring the Schema Using Reflection\n",
        "Note that Apache Spark is inferring the schema using reflection; i.e. it automaticlaly determines the schema of the data based on reviewing the JSON data."
      ],
      "metadata": {
        "id": "XpsrqMtclgRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the schema\n",
        "swimmersJSON.printSchema()"
      ],
      "metadata": {
        "id": "X7xn9Y0tlgR0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b4f1441-41bd-4224-c536-729e5c620d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- eyeColor: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "execution_count": 49
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that Spark was able to determine infer the schema (when reviewing the schema using `.printSchema`).\n",
        "\n",
        "But what if we want to programmatically specify the schema?"
      ],
      "metadata": {
        "id": "bGavPjuylgR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Programmatically Specifying the Schema\n",
        "In this case, let's specify the schema for a `CSV` text file."
      ],
      "metadata": {
        "id": "G8ABNLoxlgR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "# Generate our own CSV data\n",
        "#   This way we don't have to access the file system yet.\n",
        "stringCSVRDD = sc.parallelize([(123, 'Katie', 19, 'brown'), (234, 'Michael', 22, 'green'), (345, 'Simone', 23, 'blue')])\n",
        "\n",
        "# The schema is encoded in a string, using StructType we define the schema using various pyspark.sql.types\n",
        "schemaString = \"id name age eyeColor\"\n",
        "schema = StructType([\n",
        "    StructField(\"id\", LongType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", LongType(), True),\n",
        "    StructField(\"eyeColor\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Apply the schema to the RDD and Create DataFrame\n",
        "swimmers = spark.createDataFrame(stringCSVRDD, schema)\n",
        "\n",
        "# Creates a temporary view using the DataFrame\n",
        "swimmers.createOrReplaceTempView(\"swimmers\")"
      ],
      "metadata": {
        "id": "eAH0C80XlgR3"
      },
      "outputs": [],
      "execution_count": 50
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the schema\n",
        "#   Notice that we have redefined id as Long (instead of String)\n",
        "swimmers.printSchema()"
      ],
      "metadata": {
        "id": "PFlNJQw_lgR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27219d64-c13f-476d-a638-8e86aa9a254d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            " |-- eyeColor: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "execution_count": 51
    },
    {
      "cell_type": "code",
      "source": [
        "# %sql\n",
        "# -- Query the data\n",
        "# select * from swimmers"
      ],
      "metadata": {
        "id": "C5upG0KvlgR5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see from above, we can programmatically apply the `schema` instead of allowing the Spark engine to infer the schema via reflection.\n",
        "\n",
        "Additional Resources include:\n",
        "* [PySpark API Reference](https://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html)\n",
        "* [Spark SQL, DataFrames, and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html#programmatically-specifying-the-schema): This is in reference to Programmatically Specifying the Schema using a `CSV` file."
      ],
      "metadata": {
        "id": "lZ9Pu15_lgR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####|| SparkSession\n",
        "\n",
        "Notice that we're no longer using `sqlContext.read...` but instead `spark.read...`.  This is because as part of Spark 2.0, `HiveContext`, `SQLContext`, `StreamingContext`, `SparkContext` have been merged together into the Spark Session `spark`.\n",
        "* Entry point for reading data\n",
        "* Working with metadata\n",
        "* Configuration\n",
        "* Cluster resource management\n",
        "\n",
        "For more information, please refer to [How to use SparkSession in Apache Spark 2.0](http://bit.ly/2br0Fr1) (http://bit.ly/2br0Fr1)."
      ],
      "metadata": {
        "id": "EGuSkMTJlgR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Querying with SQL\n",
        "With DataFrames, you can start writing your queries using `Spark SQL` - a SQL dialect that is compatible with the Hive Query Language (or HiveQL)."
      ],
      "metadata": {
        "id": "x5BQc__2lgR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute SQL Query and return the data\n",
        "spark.sql(\"select * from swimmers\").show()"
      ],
      "metadata": {
        "id": "RlOUvTEtlgR-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b411af-d0fd-4d72-fb77-6c450a6f21c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+--------+\n",
            "| id|   name|age|eyeColor|\n",
            "+---+-------+---+--------+\n",
            "|123|  Katie| 19|   brown|\n",
            "|234|Michael| 22|   green|\n",
            "|345| Simone| 23|    blue|\n",
            "+---+-------+---+--------+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 52
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get the row count:"
      ],
      "metadata": {
        "id": "_bq_6c4zlgR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get count of rows in SQL\n",
        "spark.sql(\"select count(1) from swimmers\").show()"
      ],
      "metadata": {
        "id": "bPel0pyMlgR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b27151c-7b95-412e-ccb6-94e620825256"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|       3|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 53
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note, you can make use of `%sql` within the notebook cells of a Databricks notebook."
      ],
      "metadata": {
        "id": "MwhMTkNOlgSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %sql\n",
        "# -- Query all data\n",
        "# select * from swimmers"
      ],
      "metadata": {
        "id": "X9UYhscBlgSA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Query id and age for swimmers with age = 22 via DataFrame API\n",
        "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
      ],
      "metadata": {
        "id": "KL7iWe6olgSA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "567278f6-1fd5-4695-85cb-632335944e0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|234| 22|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 25
    },
    {
      "cell_type": "code",
      "source": [
        "# Query id and age for swimmers with age = 22 via DataFrame API in another way\n",
        "swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()\n"
      ],
      "metadata": {
        "id": "MqsZ3bWAlgSB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3d3aa8d-ae0f-47fc-ea01-be4a6ad8028a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|234| 22|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "source": [
        "# Query id and age for swimmers with age = 22 in SQL\n",
        "spark.sql(\"select id, age from swimmers where age = 22\").show()"
      ],
      "metadata": {
        "id": "QBfhHFEGlgSB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3da844-f506-4c17-9251-705eb85021a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|234| 22|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "source": [
        "# %sql\n",
        "# -- Query id and age for swimmers with age = 22\n",
        "# select id, age from swimmers where age = 22"
      ],
      "metadata": {
        "id": "BXi8yQ6ilgSB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "8a74cb86-47a0-463a-c81d-b3afe6246ace"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-74-fbde46e884a1>, line 2)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-74-fbde46e884a1>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    -- Query id and age for swimmers with age = 22\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Query name and eye color for swimmers with eye color starting with the letter 'b'\n",
        "spark.sql(\"select name, eyeColor from swimmers where eyeColor like 'b%'\").show()"
      ],
      "metadata": {
        "id": "wzblxdFYlgSC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "409ae4ee-77a1-4f89-e5a4-198b03d39584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+\n",
            "|  name|eyeColor|\n",
            "+------+--------+\n",
            "| Katie|   brown|\n",
            "|Simone|    blue|\n",
            "+------+--------+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": [
        "# %sql\n",
        "# -- Query name and eye color for swimmers with eye color starting with the letter 'b'\n",
        "# select name, eyeColor from swimmers where eyeColor like 'b%'"
      ],
      "metadata": {
        "id": "IepwoljglgSC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Querying with the DataFrame API\n",
        "With DataFrames, you can start writing your queries using the DataFrame API"
      ],
      "metadata": {
        "id": "XrXrZsAUlgSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the values\n",
        "swimmers.show()"
      ],
      "metadata": {
        "id": "cA3BqUzylgSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b559b2f4-cbcc-4402-d27e-04bb4f5acb10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+--------+\n",
            "| id|   name|age|eyeColor|\n",
            "+---+-------+---+--------+\n",
            "|123|  Katie| 19|   brown|\n",
            "|234|Michael| 22|   green|\n",
            "|345| Simone| 23|    blue|\n",
            "+---+-------+---+--------+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Databricks `display` command to view the data easier\n",
        "display(swimmers)"
      ],
      "metadata": {
        "id": "OJFp16DvlgSD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f331af5a-df0a-4ecc-a5b7-99549dd0c544"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, name: string, age: bigint, eyeColor: string]"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Get count of rows\n",
        "swimmers.count()"
      ],
      "metadata": {
        "id": "WCXi4ySSlgSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4c6b6d9-e254-442c-deb0-81cd368a2de1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "execution_count": 30
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the id, age where age = 22\n",
        "swimmers.select(\"id\", \"age\").filter(\"age = 22\").show()"
      ],
      "metadata": {
        "id": "qs983xMKlgSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b57d6b09-4d68-4362-d531-d3ac85d13acc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|234| 22|\n",
            "+---+---+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 31
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the name, eyeColor where eyeColor like 'b%'\n",
        "swimmers.select(\"name\", \"eyeColor\").filter(\"eyeColor like 'b%'\").show()"
      ],
      "metadata": {
        "id": "IdDfjy6SlgSF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a3af374-a73a-4097-9c70-f2669497fac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+\n",
            "|  name|eyeColor|\n",
            "+------+--------+\n",
            "| Katie|   brown|\n",
            "|Simone|    blue|\n",
            "+------+--------+\n",
            "\n"
          ]
        }
      ],
      "execution_count": 32
    },
    {
      "cell_type": "markdown",
      "source": [
        "## On-Time Flight Performance\n",
        "Query flight departure delays by State and City by joining the departure delay and join to the airport codes (to identify state and city)."
      ],
      "metadata": {
        "id": "ITkD5oGSlgSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataFrame Queries\n",
        "Let's run a flight performance using DataFrames; let's first build the DataFrames from the source datasets."
      ],
      "metadata": {
        "id": "Izsewx0nlgSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set File Paths\n",
        "flightPerfFilePath = \"/content/departuredelays.csv\"\n",
        "airportsFilePath = \"/content/airport-codes-na.txt\"\n",
        "\n",
        "# Obtain Airports dataset\n",
        "airports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\\t')\n",
        "airports.createOrReplaceTempView(\"airports\")\n",
        "\n",
        "# Obtain Departure Delays dataset\n",
        "flightPerf = spark.read.csv(flightPerfFilePath, header='true')\n",
        "flightPerf.createOrReplaceTempView(\"FlightPerformance\")\n",
        "\n",
        "# Cache the Departure Delays dataset\n",
        "flightPerf.cache()"
      ],
      "metadata": {
        "id": "p9Y0jKSSlgSK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0e43f9a-1156-4a2f-9d84-eaa46df087c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[date: string, delay: string, distance: string, origin: string, destination: string]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Query Sum of Flight Delays by City and Origin Code (for Washington State)\n",
        "spark.sql(\"select a.City, f.origin, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.State = 'WA' group by a.City, f.origin order by sum(f.delay) desc\").show()"
      ],
      "metadata": {
        "id": "lRvI2FAjlgSK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3689214-f017-41c4-e23c-557130fade49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+--------+\n",
            "|   City|origin|  Delays|\n",
            "+-------+------+--------+\n",
            "|Seattle|   SEA|159086.0|\n",
            "|Spokane|   GEG| 12404.0|\n",
            "|  Pasco|   PSC|   949.0|\n",
            "+-------+------+--------+\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# %sql\n",
        "# -- Query Sum of Flight Delays by City and Origin Code (for Washington State)\n",
        "# select a.City, f.origin, sum(f.delay) as Delays\n",
        "#   from FlightPerformance f\n",
        "#     join airports a\n",
        "#       on a.IATA = f.origin\n",
        "#  where a.State = 'WA'\n",
        "#  group by a.City, f.origin\n",
        "#  order by sum(f.delay) desc\n"
      ],
      "metadata": {
        "id": "OHT8ySq3lgSY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "flightPerf.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmFIxELQx1LZ",
        "outputId": "c03d1e02-73b1-49b8-868d-35079a4a8bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: string (nullable = true)\n",
            " |-- delay: string (nullable = true)\n",
            " |-- distance: string (nullable = true)\n",
            " |-- origin: string (nullable = true)\n",
            " |-- destination: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query Sum of Flight Delays by State (for the US)\n",
        "spark.sql(\"select a.State, sum(f.delay) as Delays from FlightPerformance f join airports a on a.IATA = f.origin where a.Country = 'USA' group by a.State \").show()"
      ],
      "metadata": {
        "id": "kQCuQKuSlgSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8222fd1a-80ae-44e9-a6c3-e7100ae40cb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+\n",
            "|State|   Delays|\n",
            "+-----+---------+\n",
            "|   SC|  80666.0|\n",
            "|   AZ| 401793.0|\n",
            "|   LA| 199136.0|\n",
            "|   MN| 256811.0|\n",
            "|   NJ| 452791.0|\n",
            "|   OR| 109333.0|\n",
            "|   VA|  98016.0|\n",
            "| NULL| 397237.0|\n",
            "|   RI|  30760.0|\n",
            "|   WY|  15365.0|\n",
            "|   KY|  61156.0|\n",
            "|   NH|  20474.0|\n",
            "|   MI| 366486.0|\n",
            "|   NV| 474208.0|\n",
            "|   WI| 152311.0|\n",
            "|   ID|  22932.0|\n",
            "|   CA|1891919.0|\n",
            "|   CT|  54662.0|\n",
            "|   NE|  59376.0|\n",
            "|   MT|  19271.0|\n",
            "+-----+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# %sql\n",
        "# -- Query Sum of Flight Delays by State (for the US)\n",
        "# select a.State, sum(f.delay) as Delays\n",
        "#   from FlightPerformance f\n",
        "#     join airports a\n",
        "#       on a.IATA = f.origin\n",
        "#  where a.Country = 'USA'\n",
        "#  group by a.State"
      ],
      "metadata": {
        "id": "kXW5yc5ulgSa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# %sql\n",
        "# -- Query Sum of Flight Delays by State (for the US)\n",
        "# select a.State, sum(f.delay) as Delays\n",
        "#   from FlightPerformance f\n",
        "#     join airports a\n",
        "#       on a.IATA = f.origin\n",
        "#  where a.Country = 'USA'\n",
        "#  group by a.State"
      ],
      "metadata": {
        "id": "KnqP9tO3lgSb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more information, please refer to:\n",
        "* [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#sql)\n",
        "* [PySpark SQL Module: DataFrame](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n",
        "* [PySpark SQL Functions Module](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions)"
      ],
      "metadata": {
        "id": "QQx3AEFflgSc"
      }
    }
  ],
  "metadata": {
    "name": "Ch4 - DataFrames",
    "notebookId": 4341522646494009,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}