{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kareemullah123456789/1112232/blob/master/pyngrok_UI_CODE_Working_with_RDDs_in_PySpark_(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "animated-appreciation",
      "metadata": {
        "id": "animated-appreciation"
      },
      "source": [
        "### Getting Setup (On Google Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "engaged-contrast",
      "metadata": {
        "id": "engaged-contrast"
      },
      "source": [
        "* Begin by installing some pip packages and the java development kit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "specified-virginia",
      "metadata": {
        "id": "specified-virginia"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark --quiet\n",
        "#!pip install -U -q PyDrive --quiet\n",
        "#!apt install openjdk-8-jdk-headless &> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "entire-laptop",
      "metadata": {
        "id": "entire-laptop"
      },
      "source": [
        "* Then set the java environmental variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Up-otH4uLK1E",
      "metadata": {
        "id": "Up-otH4uLK1E"
      },
      "outputs": [],
      "source": [
        "RDD -- Resilient Distributed Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "adjacent-archives",
      "metadata": {
        "id": "adjacent-archives"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/lib/jvm/java-11-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "homeless-beginning",
      "metadata": {
        "id": "homeless-beginning"
      },
      "source": [
        "* Then connect to a SparkSession, setting the spark ui port to `4050`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "increasing-preliminary",
      "metadata": {
        "id": "increasing-preliminary"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"films\").setMaster(\"local[*]\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZIHsBWsmM0x",
        "outputId": "625b4568-f4ae-4b52-ed23-a266af85694f"
      },
      "id": "0ZIHsBWsmM0x",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Replace \"your_auth_token\" with your actual ngrok token\n",
        "ngrok.set_auth_token(\"2mn3JmxhS2Tva5L5LPlVPg8vENL_6F56KMq9DJaoH2bRUEhBz\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQwjSfUzneyk",
        "outputId": "11cffa63-2ba1-4b8a-e380-263f81ce5e20"
      },
      "id": "OQwjSfUzneyk",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kAqQz32_nfHn"
      },
      "id": "kAqQz32_nfHn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken 2mn3JmxhS2Tva5L5LPlVPg8vENL_6F56KMq9DJaoH2bRUEhBz\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XC1utT6Onq98",
        "outputId": "7ec689fd-4b40-4d43-9579-2e8f77bc88ca"
      },
      "id": "XC1utT6Onq98",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set up Spark configuration\n",
        "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"films\").setMaster(\"local[*]\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "# Expose the Spark UI port via ngrok\n",
        "spark_ui_url = ngrok.connect(4050)\n",
        "\n",
        "print(f\"Spark UI running on: {spark_ui_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGcb_PT3n92_",
        "outputId": "368f6393-a645-4d82-e43d-5437a5a24b37"
      },
      "id": "DGcb_PT3n92_",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark UI running on: NgrokTunnel: \"https://db12-34-83-135-166.ngrok-free.app\" -> \"http://localhost:4050\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CncMGFnJncAn"
      },
      "id": "CncMGFnJncAn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GQI1qfpDndku"
      },
      "id": "GQI1qfpDndku",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OiynDe5Tn-Go"
      },
      "id": "OiynDe5Tn-Go",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Set up Spark configuration\n",
        "conf = SparkConf().set('spark.ui.port', '4050').setAppName(\"films\").setMaster(\"local[*]\")\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "# Expose the Spark UI port via ngrok\n",
        "spark_ui_url = ngrok.connect(4050)\n",
        "\n",
        "print(f\"Spark UI running on: {spark_ui_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "q_ylhZOOmNG0",
        "outputId": "cc56569f-6456-49b9-c993-ee5a9763a73e"
      },
      "id": "q_ylhZOOmNG0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:pyngrok.process.ngrok:t=2024-10-04T03:37:59+0000 lvl=eror msg=\"failed to reconnect session\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2024-10-04T03:37:59+0000 lvl=eror msg=\"session closing\" obj=tunnels.session err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "ERROR:pyngrok.process.ngrok:t=2024-10-04T03:37:59+0000 lvl=eror msg=\"terminating with error\" obj=app err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n",
            "CRITICAL:pyngrok.process.ngrok:t=2024-10-04T03:37:59+0000 lvl=crit msg=\"command failed\" err=\"authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PyngrokNgrokError",
          "evalue": "The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e086348dfcae>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Expose the Spark UI port via ngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mspark_ui_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngrok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4050\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Spark UI running on: {spark_ui_url}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(addr, proto, name, pyngrok_config, **options)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"auth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ngrok_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Creating tunnel with options: {options}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/ngrok.py\u001b[0m in \u001b[0;36mget_ngrok_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0minstall_ngrok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36mget_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_current_processes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngrok_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_start_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyngrok_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyngrok/process.py\u001b[0m in \u001b[0;36m_start_process\u001b[0;34m(pyngrok_config)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartup_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             raise PyngrokNgrokError(f\"The ngrok process errored on start: {ngrok_process.startup_error}.\",\n\u001b[0m\u001b[1;32m    399\u001b[0m                                     \u001b[0mngrok_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                                     ngrok_process.startup_error)\n",
            "\u001b[0;31mPyngrokNgrokError\u001b[0m: The ngrok process errored on start: authentication failed: Usage of ngrok requires a verified account and authtoken.\\n\\nSign up for an account: https://dashboard.ngrok.com/signup\\nInstall your authtoken: https://dashboard.ngrok.com/get-started/your-authtoken\\r\\n\\r\\nERR_NGROK_4018\\r\\n."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TO6ylkexmu2y"
      },
      "id": "TO6ylkexmu2y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uXKu2Wyqmu8T"
      },
      "id": "uXKu2Wyqmu8T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I_LxddIumvBa"
      },
      "id": "I_LxddIumvBa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5k5Z_KYHmvF0"
      },
      "id": "5k5Z_KYHmvF0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "comparative-restriction",
      "metadata": {
        "id": "comparative-restriction"
      },
      "source": [
        "* Then we need to install ngrok which will allow us to place our local spark ui on the web."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "disabled-radius",
      "metadata": {
        "id": "disabled-radius"
      },
      "outputs": [],
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
        "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
        "get_ipython().system_raw('./ngrok http 4050 &')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "proper-longer",
      "metadata": {
        "id": "proper-longer"
      },
      "source": [
        "* And finally we get a link our Spark UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "buried-blink",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buried-blink",
        "outputId": "bf9b6b8e-d0a6-4f63-e414-63ff73ac0f4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ]
        }
      ],
      "source": [
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "opposed-oliver",
      "metadata": {
        "id": "opposed-oliver"
      },
      "source": [
        "### Looking Under the Hood"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "logical-catholic",
      "metadata": {
        "id": "logical-catholic"
      },
      "source": [
        "Now  let's again create an RDD from our movie records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "charitable-assets",
      "metadata": {
        "id": "charitable-assets"
      },
      "outputs": [],
      "source": [
        "movies = ['dark knight', 'dunkirk', 'pulp fiction', 'avatar']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "optimum-terror",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "optimum-terror",
        "outputId": "cddf2e81-b698-43b2-f7dd-f23c9f5f80c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "movies_rdd = sc.parallelize(movies)\n",
        "movies_rdd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "entertaining-investor",
      "metadata": {
        "id": "entertaining-investor"
      },
      "source": [
        "And then let's capitalize the movies, and select the movies that begin with `d`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "aJa7JA_YmfVA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJa7JA_YmfVA",
        "outputId": "db67db40-261c-498f-9bc6-7cd425a6729f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dark knight', 'dunkirk', 'pulp fiction', 'avatar']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "movies_rdd.collect() #action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8roq2TrUme2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8roq2TrUme2d",
        "outputId": "e21d0a69-7ef0-43be-ce90-fd29d7e7bf85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dark knight', 'dunkirk', 'pulp fiction']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "movies_rdd.take(3) #actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7l0vwchInAiP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7l0vwchInAiP",
        "outputId": "df1d6582-3660-4828-9f9e-94aa5361b45f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Dark Knight'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "movies[0].title()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cRWXgCXsVsud",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRWXgCXsVsud",
        "outputId": "e74f2421-116f-4f29-f9ea-2f15aff55e45"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dark knight', 'dunkirk', 'pulp fiction', 'avatar']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "movies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "qDEH6-7-VsNz",
      "metadata": {
        "id": "qDEH6-7-VsNz"
      },
      "outputs": [],
      "source": [
        "transform=lambda i:i.title()\n",
        "movies_title=[transform(i) for i in movies]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uTjFpZ3vWBmJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTjFpZ3vWBmJ",
        "outputId": "dd29bc32-ad5d-4aaf-9470-2eb5502389b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dark Knight', 'Dunkirk', 'Pulp Fiction', 'Avatar']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "movies_title"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "IzfUqE4DnAGL",
      "metadata": {
        "id": "IzfUqE4DnAGL"
      },
      "outputs": [],
      "source": [
        "movies_title_rdd=movies_rdd.map(transform) ## transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "JOkxFkUEYjTw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOkxFkUEYjTw",
        "outputId": "362d254f-e3f5-4ff5-abfa-12befe0949fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dark Knight', 'Dunkirk', 'Pulp Fiction', 'Avatar']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "movies_title_rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_2rtRSkDm_2B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2rtRSkDm_2B",
        "outputId": "be9decb8-12d3-4b4c-b729-a40f786b86db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dark knight', 'dunkirk', 'pulp fiction', 'avatar']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "movies_rdd.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TPhb_Qwgm_IR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPhb_Qwgm_IR",
        "outputId": "3ab653d9-7e72-4223-f9b9-efc82a54c6a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dark knight', 'dunkirk']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "movies_rdd.filter(lambda movies : movies[0]=='d').collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "frozen-retail",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frozen-retail",
        "outputId": "4bbe03de-fe7a-44d3-e868-0ef5ea4f6475"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dark Knight', 'Dunkirk']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "movies_rdd.map(lambda movie: movie.title()).take(2)\n",
        "#transformations -- lazy transformations\n",
        "## once you apply a transformation only the function is created but it is not applied\n",
        "## you need an action to apply the transformation across your rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qRaLQWQbs8sw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "qRaLQWQbs8sw",
        "outputId": "07d4e115-f3b3-4fb5-de97-704d67fb3b4b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rdd2' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-83517eaf6d43>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'rdd2' is not defined"
          ]
        }
      ],
      "source": [
        "rdd2.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qN9DQ_G-a7O_"
      },
      "id": "qN9DQ_G-a7O_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "M0ex3LfSsaAT",
      "metadata": {
        "id": "M0ex3LfSsaAT"
      },
      "outputs": [],
      "source": [
        "rdd1=movies_rdd.map(lambda movies :movies.title()).collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ekqK6oMssl9e",
      "metadata": {
        "id": "ekqK6oMssl9e"
      },
      "outputs": [],
      "source": [
        "rdd2=movies_rdd.map(lambda movies : movies.title())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# from pyngrok import ngrok\n",
        "\n",
        "# # Step 1: Create a Spark session\n",
        "# spark = SparkSession.builder \\\n",
        "#     .appName(\"example\") \\\n",
        "#     .config(\"spark.ui.port\", \"4050\") \\\n",
        "#     .getOrCreate()\n",
        "\n",
        "# # Step 2: Expose the Spark UI using ngrok\n",
        "# spark_ui_url = ngrok.connect(4050)\n",
        "\n",
        "# print(f\"Access the Spark UI via this URL: {spark_ui_url}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjlqowAdsqAa",
        "outputId": "094b98d4-76b4-4821-c825-42511905f15d"
      },
      "id": "WjlqowAdsqAa",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access the Spark UI via this URL: NgrokTunnel: \"https://7cfe-34-32-235-13.ngrok-free.app\" -> \"http://localhost:4050\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Read the CSV file\n",
        "df_emp = spark.read.csv('/content/sales_data.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Step 2: Perform some operations (e.g., showing the first few rows)\n",
        "df_emp.show(15)\n",
        "\n",
        "# Step 3: Perform more transformations or actions (e.g., count rows)\n",
        "df_emp.count()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ugTNTlKsqMN",
        "outputId": "fd66294d-d5d7-433d-a0c6-c6a81b4b7afe"
      },
      "id": "5ugTNTlKsqMN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+--------+----------+---------+---------+---------------+----------------+--------+\n",
            "|ORDERNUMBER|QUANTITYORDERED|PRICEEACH|ORDERLINENUMBER|  SALES|      ORDERDATE| STATUS|QTR_ID|MONTH_ID|YEAR_ID|PRODUCTLINE|MSRP|PRODUCTCODE|        CUSTOMERNAME|           PHONE|        ADDRESSLINE1|ADDRESSLINE2|         CITY|   STATE|POSTALCODE|  COUNTRY|TERRITORY|CONTACTLASTNAME|CONTACTFIRSTNAME|DEALSIZE|\n",
            "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+--------+----------+---------+---------+---------------+----------------+--------+\n",
            "|      10107|             30|     95.7|              2| 2871.0| 2/24/2003 0:00|Shipped|     1|       2|   2003|Motorcycles|  95|   S10_1678|   Land of Toys Inc.|      2125557818|897 Long Airport ...|        NULL|          NYC|      NY|     10022|      USA|       NA|             Yu|            Kwai|   Small|\n",
            "|      10121|             34|    81.35|              5| 2765.9|  5/7/2003 0:00|Shipped|     2|       5|   2003|Motorcycles|  95|   S10_1678|  Reims Collectables|      26.47.1555|  59 rue de l'Abbaye|        NULL|        Reims|    NULL|     51100|   France|     EMEA|        Henriot|            Paul|   Small|\n",
            "|      10134|             41|    94.74|              2|3884.34|  7/1/2003 0:00|Shipped|     3|       7|   2003|Motorcycles|  95|   S10_1678|     Lyon Souveniers|+33 1 46 62 7555|27 rue du Colonel...|        NULL|        Paris|    NULL|     75508|   France|     EMEA|       Da Cunha|          Daniel|  Medium|\n",
            "|      10145|             45|    83.26|              6| 3746.7| 8/25/2003 0:00|Shipped|     3|       8|   2003|Motorcycles|  95|   S10_1678|   Toys4GrownUps.com|      6265557265|  78934 Hillside Dr.|        NULL|     Pasadena|      CA|     90003|      USA|       NA|          Young|           Julie|  Medium|\n",
            "|      10159|             49|    100.0|             14|5205.27|10/10/2003 0:00|Shipped|     4|      10|   2003|Motorcycles|  95|   S10_1678|Corporate Gift Id...|      6505551386|     7734 Strong St.|        NULL|San Francisco|      CA|      NULL|      USA|       NA|          Brown|           Julie|  Medium|\n",
            "|      10168|             36|    96.66|              1|3479.76|10/28/2003 0:00|Shipped|     4|      10|   2003|Motorcycles|  95|   S10_1678|Technics Stores Inc.|      6505556809|   9408 Furth Circle|        NULL|   Burlingame|      CA|     94217|      USA|       NA|         Hirano|            Juri|  Medium|\n",
            "|      10180|             29|    86.13|              9|2497.77|11/11/2003 0:00|Shipped|     4|      11|   2003|Motorcycles|  95|   S10_1678|Daedalus Designs ...|      20.16.1555|184, chausse de T...|        NULL|        Lille|    NULL|     59000|   France|     EMEA|          Rance|         Martine|   Small|\n",
            "|      10188|             48|    100.0|              1|5512.32|11/18/2003 0:00|Shipped|     4|      11|   2003|Motorcycles|  95|   S10_1678|        Herkku Gifts|   +47 2267 3215|Drammen 121, PR 7...|        NULL|       Bergen|    NULL|    N 5804|   Norway|     EMEA|         Oeztan|          Veysel|  Medium|\n",
            "|      10201|             22|    98.57|              2|2168.54| 12/1/2003 0:00|Shipped|     4|      12|   2003|Motorcycles|  95|   S10_1678|     Mini Wheels Co.|      6505555787|5557 North Pendal...|        NULL|San Francisco|      CA|      NULL|      USA|       NA|         Murphy|           Julie|   Small|\n",
            "|      10211|             41|    100.0|             14|4708.44| 1/15/2004 0:00|Shipped|     1|       1|   2004|Motorcycles|  95|   S10_1678|    Auto Canal Petit|  (1) 47.55.6555|   25, rue Lauriston|        NULL|        Paris|    NULL|     75016|   France|     EMEA|        Perrier|       Dominique|  Medium|\n",
            "|      10223|             37|    100.0|              1|3965.66| 2/20/2004 0:00|Shipped|     1|       2|   2004|Motorcycles|  95|   S10_1678|Australian Collec...|    03 9520 4555|   636 St Kilda Road|     Level 3|    Melbourne|Victoria|      3004|Australia|     APAC|       Ferguson|           Peter|  Medium|\n",
            "|      10237|             23|    100.0|              7|2333.12|  4/5/2004 0:00|Shipped|     2|       4|   2004|Motorcycles|  95|   S10_1678|     Vitachrome Inc.|      2125551500|   2678 Kingston Rd.|   Suite 101|          NYC|      NY|     10022|      USA|       NA|          Frick|         Michael|   Small|\n",
            "|      10251|             28|    100.0|              2|3188.64| 5/18/2004 0:00|Shipped|     2|       5|   2004|Motorcycles|  95|   S10_1678|Tekni Collectable...|      2015559350|       7476 Moss Rd.|        NULL|       Newark|      NJ|     94019|      USA|       NA|          Brown|         William|  Medium|\n",
            "|      10263|             34|    100.0|              2|3676.76| 6/28/2004 0:00|Shipped|     2|       6|   2004|Motorcycles|  95|   S10_1678|     Gift Depot Inc.|      2035552570| 25593 South Bay Ln.|        NULL|  Bridgewater|      CT|     97562|      USA|       NA|           King|           Julie|  Medium|\n",
            "|      10275|             45|    92.83|              1|4177.35| 7/23/2004 0:00|Shipped|     3|       7|   2004|Motorcycles|  95|   S10_1678|   La Rochelle Gifts|      40.67.8555|67, rue des Cinqu...|        NULL|       Nantes|    NULL|     44000|   France|     EMEA|        Labrune|          Janine|  Medium|\n",
            "+-----------+---------------+---------+---------------+-------+---------------+-------+------+--------+-------+-----------+----+-----------+--------------------+----------------+--------------------+------------+-------------+--------+----------+---------+---------+---------------+----------------+--------+\n",
            "only showing top 15 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2823"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#use same dataframe to apply groupby and sum operation\n",
        "sum_df=df_emp.groupBy('PRODUCTLINE').sum('SALES')\n",
        "sum_df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsDu-VCYvj4l",
        "outputId": "872d9834-a7ee-4b01-f6f7-b46092325ded"
      },
      "id": "KsDu-VCYvj4l",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+------------------+\n",
            "|     PRODUCTLINE|        sum(SALES)|\n",
            "+----------------+------------------+\n",
            "|     Motorcycles|1166388.3400000003|\n",
            "|    Vintage Cars|1903150.8399999992|\n",
            "|           Ships|         714437.13|\n",
            "|Trucks and Buses|1127789.8399999996|\n",
            "|    Classic Cars| 3919615.659999997|\n",
            "+----------------+------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qjefqPLNvkAa"
      },
      "id": "qjefqPLNvkAa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q571ZDf0sqX-"
      },
      "id": "Q571ZDf0sqX-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_Hfe4f86s2Gw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hfe4f86s2Gw",
        "outputId": "c5846be8-017c-48c3-8cc7-d34c0071d192"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pyspark.rdd.PipelinedRDD"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(rdd2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ej56XIDBryHk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej56XIDBryHk",
        "outputId": "b93d9eab-c628-4098-82a3-10c6ca860b04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Dark Knight', 'Dunkirk', 'Pulp Fiction', 'Avatar']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.map(lambda movies :movies.title()).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "knowing-bleeding",
      "metadata": {
        "id": "knowing-bleeding"
      },
      "source": [
        "Now as we know, Spark will partition the dataset across the cores of the executors, and then map through the records in parallel, returning all of the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bronze-friend",
      "metadata": {
        "id": "bronze-friend"
      },
      "source": [
        "> <img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/parallel.png?raw=1\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accredited-processor",
      "metadata": {
        "id": "accredited-processor"
      },
      "source": [
        "Now let's change the function so that this time, instead of returning all of the results, we just return the first result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "floating-louis",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "floating-louis",
        "outputId": "25d99e1c-0ab2-4ca2-a35d-2ec24f4ce1c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Dark Knight']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.map(lambda movie: movie.title()).take(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "expired-leave",
      "metadata": {
        "id": "expired-leave"
      },
      "source": [
        "Now if we think about, this previous step, here we would not have to map through all of the steps just to return a single result.  And it turns out if we look at Spark, we can see that even though the dataset was distributed -- it only needed to perform work on a single partition to return one result."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accepted-drink",
      "metadata": {
        "id": "accepted-drink"
      },
      "source": [
        "> <img src=\"https://github.com/jigsawlabs-student/pyspark-rdds/blob/main/individual_task.png?raw=1\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "herbal-future",
      "metadata": {
        "id": "herbal-future"
      },
      "source": [
        "This ability, to see the end result that needs to be returned, and to work efficiently to only take the needed steps to return those results, is a valuable feature when working with large datasets.  And we can better see how Spark accomplishes it in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fitting-publisher",
      "metadata": {
        "id": "fitting-publisher"
      },
      "source": [
        "### A little experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "electronic-engagement",
      "metadata": {
        "id": "electronic-engagement"
      },
      "source": [
        "If we run the code below, notice that nothing is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "powered-bikini",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "powered-bikini",
        "outputId": "b98d0cb0-64a3-4796-fbd5-8c872d0114e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[6] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "movies_rdd.map(lambda movie: movie.title())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "published-surprise",
      "metadata": {
        "id": "published-surprise"
      },
      "source": [
        "And even if we chain the map and the filter methods, still nothing is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "exceptional-saturn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exceptional-saturn",
        "outputId": "c299062f-c379-44f0-d17f-99fed961f909"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dark Knight', 'Dunkirk']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "movies_rdd.map(lambda movie: movie.title()).filter(lambda movie: movie[0] == 'D').collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fossil-toner",
      "metadata": {
        "id": "fossil-toner"
      },
      "source": [
        "It's only when we add a collect function on the end, will some data be returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "variable-print",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "variable-print",
        "outputId": "ac8fd04d-d156-4848-8754-4d244b9426ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Dark Knight', 'Dunkirk']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.filter(lambda movie: movie[0] == 'd').map(lambda movie: movie.title()).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "unlimited-civilian",
      "metadata": {
        "id": "unlimited-civilian"
      },
      "source": [
        "So above, nothing was returned when we ran the `map` and `collect` functions, because when we only executed those functions, Spark did not actually act on the data.  Then in the third line we finally did act on the data.  We told Spark that we want to both transform, and filter the data, and then return all of the results.  \n",
        "\n",
        "So it's only when we called the `collect` function that Spark's driver determined the tasks to then send off to the executors and return the results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "olive-format",
      "metadata": {
        "id": "olive-format"
      },
      "source": [
        "### Transformations and Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "governmental-definition",
      "metadata": {
        "id": "governmental-definition"
      },
      "source": [
        "So above we can see that the functions `map` and `filter` do not actually perform any work on our data.  Instead steps are only kicked off when we call the `collect` method.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "flush-remains",
      "metadata": {
        "id": "flush-remains"
      },
      "source": [
        "In Spark, the methods that kick off tasks and return results are called **actions** (eg. collect).  And methods like `map` and `transform` that do not are called **transformations**.  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "noted-exposure",
      "metadata": {
        "id": "noted-exposure"
      },
      "source": [
        "1. Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "protected-biology",
      "metadata": {
        "id": "protected-biology"
      },
      "source": [
        "So we already saw that transformations include `map` and `filter`, and our transformations do not actually return results to our users.  Here's a couple other transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imported-writer",
      "metadata": {
        "id": "imported-writer"
      },
      "source": [
        "* sample"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intense-swedish",
      "metadata": {
        "id": "intense-swedish"
      },
      "source": [
        "The `sample` method allows us to take a random sample from our dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "major-familiar",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "major-familiar",
        "outputId": "e819989e-0f32-46b2-bf62-b5494b1931ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['dunkirk', 'avatar']"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.sample(fraction = 0.5, withReplacement = False).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "considered-copper",
      "metadata": {
        "id": "considered-copper"
      },
      "source": [
        "> Notice that it does not return any data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "supreme-idaho",
      "metadata": {
        "id": "supreme-idaho"
      },
      "source": [
        "* distinct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "handed-tennis",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "handed-tennis",
        "outputId": "40acbc8c-01dd-4857-f011-5915450a40c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['dark knight', 'dunkirk', 'pulp fiction', 'avatar']"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.distinct().collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "frequent-accounting",
      "metadata": {
        "id": "frequent-accounting"
      },
      "source": [
        "> Distinct finds the unique results.  Notice that it also does not return data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sustainable-tension",
      "metadata": {
        "id": "sustainable-tension"
      },
      "source": [
        "Finally, we have already seen `map`, which provides a one to one transformation of our records, and `select` which filters our data.  In each case, our transformations do not return data to us."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "endless-columbus",
      "metadata": {
        "id": "endless-columbus"
      },
      "source": [
        "2. Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "happy-parts",
      "metadata": {
        "id": "happy-parts"
      },
      "source": [
        "Actions are a bit more about the end result.  So far we've learned about `collect`, which returns *all* of the results of a series of transformations.  \n",
        "\n",
        "* Take\n",
        "\n",
        "We've also seen `take`, which limits our results to a subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "radical-spotlight",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "radical-spotlight",
        "outputId": "0c8093ec-1828-4dd9-f3fd-d2da8a05506a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['dark knight', 'dunkirk']"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "movies_rdd.distinct().take(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hindu-diagram",
      "metadata": {
        "id": "hindu-diagram"
      },
      "source": [
        "> So `take` is similar to the `LIMIT` function in SQL. Notice that here our records are returned."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "active-lending",
      "metadata": {
        "id": "active-lending"
      },
      "source": [
        "* Count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "mathematical-advice",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mathematical-advice",
        "outputId": "0abdef4f-e947-4ff7-83b7-3e381b4b6734"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "movies_rdd.distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pyspark_job.py\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def run_pyspark_job():\n",
        "    spark = SparkSession.builder.appName(\"Scheduled PySpark Job\").getOrCreate()\n",
        "    df = spark.read.csv(\"/path/to/data.csv\", header=True, inferSchema=True)\n",
        "    filtered_df = df.filter(df['age'] > 25)\n",
        "    selected_df = filtered_df.select(\"name\", \"age\")\n",
        "    selected_df.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pyspark_job()\n"
      ],
      "metadata": {
        "id": "MmVxNN00a8_h",
        "outputId": "5a685b0e-057f-4db0-e534-2a24a84defcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "id": "MmVxNN00a8_h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/path/to/data.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-dea8d87eb3ea>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mrun_pyspark_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-dea8d87eb3ea>\u001b[0m in \u001b[0;36mrun_pyspark_job\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_pyspark_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Scheduled PySpark Job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/path/to/data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mfiltered_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'age'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mselected_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"age\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/path/to/data.csv."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "asian-hepatitis",
      "metadata": {
        "id": "asian-hepatitis"
      },
      "source": [
        "Count simply counts the results."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}